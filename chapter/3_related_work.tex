\chapter{関連研究}
\label{chap:related_work}

本章では、強化学習の基本的な概念と、マルチエージェント強化学習、およびポリシー蒸留に関する関連研究について述べる。

\section{強化学習}
\subsection{強化学習の基本}
強化学習とは、エージェントが環境と相互作用する中で、報酬の期待値を最大化するような行動方策を獲得するための機械学習の手法である。この章では、強化学習の基本的な概念とその定式化について説明する。

\begin{itemize}
  \item 状態 (State) $s_t$: エージェントが環境から観測する情報を表す変数。時間$t$における環境の状況を示す。
  \item 行動 (Action) $a_t$: エージェントが状態$s_t$に基づいて選択する行動。エージェントは行動$a_t$を環境に対して実行する。
  \item 観測 (Observation) $o_t$: エージェントが環境から受け取る情報。部分観測可能な環境では、エージェントは完全な状態$s_t$を観測できず、観測$o_t$を通じて環境の情報を得る。
  \item 報酬 (Reward) $r_t$: エージェントが状態$s_t$と行動$a_t$を取った結果として環境から受け取るフィードバック。エージェントの目的は、将来の報酬の期待値を最大化することである。
  \item 方策 (Policy) $\pi(a_t|s_t)$: エージェントが状態$s_t$において行動$a_t$を選択する確率分布。方策はエージェントの行動選択のルールを定義する。
    
  \item Value Function(価値関数) $V^{\pi}(s)$: 方策$\pi$に従って行動した場合の、状態$s$から得られる将来の累積報酬の期待値を表す関数。価値関数は、エージェントがどれだけ良い状態にいるかを評価するために使用される。
  
  \item Q-Function(行動価値関数) $Q^{\pi}(s,a)$: 方策$\pi$に従って行動した場合の、状態$s$で行動$a$を選択したときに得られる将来の累積報酬の期待値を表す関数。行動価値関数は、特定の行動がどれだけ良いかを評価するために使用される。
  
\end{itemize}


\subsection{Actor-Critic法}

\subsection{PPO}
ここでは、最も広く使われている強化学習アルゴリズムであり、本研究の基礎にもなっているProximal Policy Optimization(PPO)について説明する。PPOは、方策勾配法に基づくアルゴリズムであり、方策の勾配を近似的に計算し、方策の更新幅にも制約を設けることで、安定した学習を実現している。

\subsection{RNN}

\subsection{ポリシー蒸留}

\section{Transformer}
\subsection{Self-Attention}



強化学習の文脈では、時系列方向の特徴抽出にTransformerを用いるDecision Transformerが一般的であるが、本研究では、Multi-agent Transformerに基づき、エージェント間の関係性の特徴抽出にTransformerを用いる。

\section{マルチエージェント強化学習}

\subsection{Centralized Training with Decentralized Execution(CTDE)}

Centralized Training with Decentralized Execution(CTDE)は、マルチエージェント強化学習において最も一般的な枠組みである。学習時には全てのエージェントの観測情報と行動情報を統合して、中央集権的に方策や価値関数を学習する。一方、実行時には各エージェントが自らの局所観測に基づいて独立して行動を選択する。このアプローチにより、学習時の情報共有によって協調的な行動方策を獲得できる一方で、実行時には通信制約のある環境下でも動作可能となる。

\subsection{Qmix}


\subsection{MAPPO}


\subsection{Multi-agent Transformer}

Multi-agent Transformerは、Transformerアーキテクチャをマルチエージェント強化学習に適用した手法である。各エージェントの観測情報を入力として、Self-Attention機構を用いてエージェント間の関係性を学習し、行動生成において他のエージェントのアクションを考慮して自己回帰的に行動を生成することで、高度な協調戦略を実現する。

% タスクでの優れた性能に触れる。

この手法の問題点として、中央実行つまり、全エージェントの観測情報を統合して行動を決定することが前提となっているため、通信制約のある環境下では適用が困難である点が挙げられる。

